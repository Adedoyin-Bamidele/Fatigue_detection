{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc_AvGd_1Nk1"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"rihabkaci99/fatigue-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for root, dirs, files in os.walk(path):\n",
        "    print(root, len(files))\n"
      ],
      "metadata": {
        "id": "mJxWZqMo1mTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "fatigue_shw = path + '/Data/Fatigue'\n",
        "\n",
        "files = os.listdir(fatigue_shw)[:10]\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i, img_name in enumerate(files):\n",
        "  img = Image.open(os.path.join(fatigue_shw, img_name))\n",
        "  plt.subplot(2\n",
        "              , 5, i+1)\n",
        "  plt.imshow(img)\n",
        "  plt.axis('off')\n",
        "  plt.title(img_name)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "v_-8qJXJ4Fmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_fatigue = path + '/Data/NonFatigue'\n",
        "files2 = os.listdir(non_fatigue)[:10]\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i, img_name in enumerate(files2):\n",
        "  img = Image.open(os.path.join(non_fatigue, img_name))\n",
        "  plt.subplot(2, 5, i+1)\n",
        "  plt.imshow(img)\n",
        "  plt.axis('off')\n",
        "  plt.title(img_name)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TKFzk0MU415s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "9-nZjJ5S6RbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = datasets.ImageFolder(root = path + '/Data')\n",
        "train_set = datasets.ImageFolder(root = path + '/Data', transform = train_transform)\n",
        "test_set = datasets.ImageFolder(root = path + '/Data', transform = test_transform)"
      ],
      "metadata": {
        "id": "HJOd1Z9R9dxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = int(0.8* len(data))\n",
        "test = len(data)- train\n",
        "train_indices, test_indices = random_split(range(len(data)), [train, test])"
      ],
      "metadata": {
        "id": "GfMMmEtA-0g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = Subset(train_set, train_indices.indices)\n",
        "test_data = Subset(test_set, test_indices.indices)\n",
        "\n"
      ],
      "metadata": {
        "id": "z5EJboVd_uPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data, batch_size = 64, shuffle = True)\n",
        "test_loader = DataLoader(test_data, batch_size = 64, shuffle = False)"
      ],
      "metadata": {
        "id": "O7bT-xlNBsNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "id": "BRsr7kuNB2Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, input_channel, output_channel, classes):\n",
        "    super(CNN, self).__init__()\n",
        "    layers=[]\n",
        "    prev_ch = input_channel\n",
        "    for out_ch in output_channel:\n",
        "      layers.append(nn.Conv2d(prev_ch, out_ch, kernel_size=3, stride=1, padding=1))\n",
        "      layers.append(nn.ReLU())\n",
        "      layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "      prev_ch = out_ch\n",
        "      self.features = nn.Sequential(*layers)\n",
        "\n",
        "      self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "      self.classifier = nn.Sequential(\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(prev_ch*1*1, 512),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.5),\n",
        "          nn.Linear(512, 128),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.5),\n",
        "          nn.Linear(128, classes)\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = self.pool(x)\n",
        "    x = self.classifier(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "Q0NDBkDXDBoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import torch\n",
        "import torch.nn as nn\n",
        " # You can adjust the number of epochs for hyperparameter tuning\n"
      ],
      "metadata": {
        "id": "aSTgU70XCWtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, criterion, optimizer):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for batch, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    total += labels.size(0)\n",
        "\n",
        "    correct += predicted.eq(labels).sum().item()\n",
        "  train_loss = running_loss / len(train_loader)\n",
        "  train_acc = 100. * correct / total\n",
        "  if batch % 100 == 0:\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "  return train_loss, train_acc\n",
        "\n"
      ],
      "metadata": {
        "id": "QqbkWXfQHR3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader, criterion):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      running_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      total += labels.size(0)\n",
        "      correct += predicted.eq(labels).sum().item()\n",
        "  test_loss = running_loss / len(test_loader)\n",
        "  test_acc = 100. * correct / total\n",
        "  print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
        "  return test_loss, test_acc"
      ],
      "metadata": {
        "id": "hfDegwTvKp3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "  n_conv = trial.suggest_int(\"n_conv\", 1, 4)\n",
        "\n",
        "  base_ch = trial.suggest_categorical(\"base_ch\", [16, 32, 48, 64])\n",
        "\n",
        "  channels_list = [int(base_ch * (2 ** i)) for i in range(n_conv)]\n",
        "  lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
        "\n",
        "  model = CNN(input_channel=3, output_channel=channels_list, classes=len(data.classes))\n",
        "\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  num_epochs = 5\n",
        "  for epoch in range(num_epochs):\n",
        "      train(model, train_loader, criterion, optimizer)\n",
        "\n",
        "\n",
        "  test_loss, accuracy = test(model, test_loader, criterion)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "urGZlmVKyk7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=5, timeout=None)"
      ],
      "metadata": {
        "id": "u-46aVkTLdA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WeP66lBrHRKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "study = study.best_params"
      ],
      "metadata": {
        "id": "9aNkczyR70Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study"
      ],
      "metadata": {
        "id": "bpmFlUhz745O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_conv_best = study['n_conv']\n",
        "base_ch_best = study['base_ch']\n",
        "\n",
        "channels_list_best = [int(base_ch_best * (2 ** i)) for i in range(n_conv_best)]\n",
        "\n",
        "model = CNN(\n",
        "    input_channel=3,\n",
        "    output_channel=channels_list_best,\n",
        "    classes=len(data.classes)\n",
        ")"
      ],
      "metadata": {
        "id": "yxvRlB_I8OSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=study['lr'])\n",
        "\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "  train(model=model, train_loader=train_loader, criterion=criterion, optimizer=optimizer)\n",
        "  test(model=model, test_loader=test_loader, criterion=criterion)"
      ],
      "metadata": {
        "id": "mFICgVQi8o8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"best_cnn_model.pt\")\n"
      ],
      "metadata": {
        "id": "Vci0QOIuH2JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n"
      ],
      "metadata": {
        "id": "6Gry3ry3IGxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "metrics=[]\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    metrics.append([accuracy, precision, recall, f1])\n",
        "    metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "29NnollmIUNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_metrics(all_labels, all_preds)"
      ],
      "metadata": {
        "id": "QXOGyat0I4SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "APY0OpLxI9oS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}